# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J0nRtRnRbNiPlDiVxOQcGqhBl0xtBTUX
"""

import torch
import json
import os
import matplotlib.pyplot as plt
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    # BitsAndBytesConfig, # <-- Â∑≤ÁßªÈô§
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
)
from datasets import load_dataset, concatenate_datasets
from transformers.trainer_callback import TrainerCallback
# „ÄêÂ∑≤‰øÆÂ§ç„ÄëÂØºÂÖ• PeftModel
from peft import LoraConfig, get_peft_model, PeftModel
from tqdm import tqdm
import warnings
from torch.utils.data import DataLoader
from torch.optim import AdamW

# Suppress warnings
warnings.filterwarnings("ignore")

# --- 1. Configuration ---
MODEL_NAME = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
HOTPOT_DATASET_NAME = "hotpot_qa"
HOTPOT_DATASET_CONFIG = "distractor"
MATH_DATASET_NAME = "qwedsacf/competition_math"
RESULTS_DIR = "temporary/"

# „ÄêÊ£ÄÊü•ÁÇπË∑ØÂæÑ 1 - ÂØπÁÖßÁªÑ„Äë
JOINT_ADAPTER_PATH = os.path.join(RESULTS_DIR, "joint_adapter_llama_fp32")

# „ÄêÊ£ÄÊü•ÁÇπË∑ØÂæÑ 2 - ÂÆûÈ™åÁªÑ Phase 1 (MATH)„Äë
TASK_A_ADAPTER_PATH = os.path.join(RESULTS_DIR, "math_adapter_llama_fp32")


# --- VRAM-Saving Config ---
MAX_SEQ_LENGTH = 2048
# „ÄêBUG ‰øÆÂ§ç„ÄëÈôç‰Ωé BS ‰ª•ÈÄÇÂ∫î FP32
PER_DEVICE_BS = 8
GRAD_ACC_STEPS = 1 # (ÊúâÊïàÊâπÈáèÂ§ßÂ∞è‰ªçÁÑ∂ÊòØ 8 * 4 = 32)

# --- Experiment Config ---
N_TRAIN_EXAMPLES = 4000
N_VAL_EXAMPLES = 400
JOINT_EPOCHS = 2
TASK_A_EPOCHS = 2 # For Math 
TASK_B_EPOCHS = 100 # For HotpotQA
alphas = [0, 0.05, 0.1, 0.15, 0.2]

# --- 2. Utility Functions (Data Formatting - Llama Chat Style) ---
def format_hotpot_qa(example):
    """Formats HotpotQA data into a Llama-chat-style prompt."""
    context = " ".join(["".join(s) for s in example["context"]["sentences"]])
    question = example["question"]
    answer = example["answer"]

    text = (
        f"<s>[INST] You are a helpful assistant. Use the following context to "
        f"answer the question. Context: {context}\n\nQuestion: {question} [/INST] "
        f"Answer: {answer}</s>"
    )
    return text

def format_math(example):
    """Formats MATH data into a Llama-chat-style prompt."""
    problem = example["problem"]
    solution = example["solution"]

    text = (
        f"<s>[INST] You are a math expert. Solve the following math problem. "
        f"Show your work.\nProblem: {problem} [/INST] "
        f"Solution: {solution}</s>"
    )
    return text

def filter_by_length(example, tokenizer, formatter):
    """
    Âè™Ê£ÄÊü•ÈïøÂ∫¶„ÄÇËøîÂõû True (‰øùÁïô) Êàñ False (‰∏¢ÂºÉ)„ÄÇ
    """
    text = formatter(example)
    tokenized = tokenizer(text, max_length=MAX_SEQ_LENGTH + 1, truncation=False, padding=False)
    return len(tokenized['input_ids']) <= MAX_SEQ_LENGTH

# „ÄêBUG ‰øÆÂ§ç„ÄëËøôÊòØ‰øÆÂ§ç‰∫Ü 1.7 Loss ÈóÆÈ¢ò Âíå ValueError ÁöÑ Preprocess ÂáΩÊï∞
def preprocess(example, tokenizer, formatter):
    """
    „ÄêÂ∑≤‰øÆÊ≠£„Äë
    Ê†ºÂºèÂåñÊñáÊú¨ÔºåÂ∫îÁî®ÊçüÂ§±Êé©Á†ÅÔºåÂπ∂Â°´ÂÖÖÂà∞ÊúÄÂ§ßÈïøÂ∫¶„ÄÇ
    """
    text = formatter(example)
    tokenized = tokenizer(
        text,
        max_length=MAX_SEQ_LENGTH,
        truncation=True,
        padding="max_length", # ‰øÆÂ§ç ValueError
    )
    labels = tokenized["input_ids"].copy()
    inst_token_id = tokenizer.convert_tokens_to_ids("]")

    split_point = -1
    for i in range(len(tokenized["input_ids"]) - 1, -1, -1):
        if tokenized["input_ids"][i] == inst_token_id:
            split_point = i + 1
            break

    if split_point == -1:
        return {}

    for i in range(split_point):
        labels[i] = -100

    tokenized["labels"] = labels
    return tokenized

# --- 3. Model Loading („ÄêÈáçÊûÑ„Äë) ---

def get_model_and_tokenizer_base():
    """
    Âè™Âä†ËΩΩ FP16 TinyLlama Âü∫Á°ÄÊ®°ÂûãÂíå Tokenizer„ÄÇ
    """
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        torch_dtype=torch.float16, # <-- Âä†ËΩΩÊó∂‰ªçÁÑ∂Áî® FP16 (ËäÇÁúÅ RAM)Ôºå‰ΩÜËÆ≠ÁªÉ‰ºöÊòØ FP32
        device_map="auto",
        trust_remote_code=True,
    )
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"

    # Âú®Âü∫Á°ÄÊ®°Âûã‰∏äÂêØÁî®Ê¢ØÂ∫¶Ê£ÄÊü•ÁÇπ
    model.gradient_checkpointing_enable()

    return model, tokenizer

def get_lora_config():
    """
    Âè™ÂÆö‰πâ LoRA ÈÖçÁΩÆ„ÄÇ
    """
    return LoraConfig(
        r=8,
        lora_alpha=16,
        target_modules=[
            "q_proj",
            "k_proj",
            "v_proj",
            "o_proj",
        ],
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM",
    )
def manual_evaluate(model, dataloader, device):
    """
    Âú®ÁªôÂÆö dataloader ‰∏äÊâãÂä®ËøêË°åËØÑ‰º∞„ÄÇ
    """
    model.eval()  # <--- ËÆæÁΩÆ‰∏∫ËØÑ‰º∞Ê®°Âºè
    total_loss = 0
    total_steps = 0
    with torch.no_grad(): # <--- Á¶ÅÁî®Ê¢ØÂ∫¶ËÆ°ÁÆó
        for batch in tqdm(dataloader, desc="Evaluating", leave=False):
            # Â∞ÜÊâπÊ¨°ÁßªÂä®Âà∞Ê®°ÂûãÊâÄÂú®ÁöÑËÆæÂ§á
            batch = {k: v.to(device) for k, v in batch.items() if k in ["input_ids", "attention_mask", "labels"]}

            outputs = model(**batch)
            loss = outputs.loss

            total_loss += loss.item()
            total_steps += 1

    model.train() # <--- „ÄêÈáçË¶Å„ÄëÂ∞ÜÊ®°ÂûãËÆæÁΩÆÂõûËÆ≠ÁªÉÊ®°Âºè
    return total_loss / total_steps

class SmoothedLossTrainer(Trainer):
    def __init__(self, *args, alpha=0.05, **kwargs):
        super().__init__(*args, **kwargs)
        self.alpha = alpha
        self.prev_loss = None

    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        loss, outputs = None, None
        # standard HF loss only, no logits returned
        return_value = super().compute_loss(
            model,
            inputs,
            return_outputs=return_outputs,
            num_items_in_batch=num_items_in_batch
        )
        if return_outputs:
          loss, outputs = return_value
        else:
          loss = return_value
        if self.prev_loss is None:
            self.prev_loss = loss.detach()
        smooth_loss = (1 - self.alpha) * loss + self.alpha * self.prev_loss
        self.prev_loss = loss.detach()
        return (smooth_loss, outputs) if return_outputs else smooth_loss
    
# --- 4. Main Experiment Logic ---
def main(alpha):
    if not os.path.exists(RESULTS_DIR):
        os.makedirs(RESULTS_DIR)

    print(f"--- Loading Base Model & Tokenizer ---")
    base_model, tokenizer = get_model_and_tokenizer_base()

    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

    # --- Load and Process Datasets ---
    print(f"\n--- Loading and Preprocessing Datasets (This may take a while) ---")

    # Task B: HotpotQA
    raw_hotpot = load_dataset(HOTPOT_DATASET_NAME, HOTPOT_DATASET_CONFIG)
    hotpot_train = raw_hotpot["train"].shuffle(seed=42).select(range(N_TRAIN_EXAMPLES))
    hotpot_val = raw_hotpot["validation"].shuffle(seed=42).select(range(N_VAL_EXAMPLES))

    print(f"Tokenizing and filtering HotpotQA...")
    import pdb
    pdb.set_trace()
    hotpot_train_tokenized = hotpot_train.filter(
        lambda x: filter_by_length(x, tokenizer, format_hotpot_qa),
        batched=False,
    )
    
    hotpot_train_tokenized = hotpot_train_tokenized.map(
        lambda x: preprocess(x, tokenizer, format_hotpot_qa),
        batched=False,
    )
    
    hotpot_train_tokenized.filter(lambda example: len(example) > 0)

    hotpot_val_tokenized = hotpot_val.filter(
        lambda x: filter_by_length(x, tokenizer, format_hotpot_qa),
        batched=False,
    ).map(
        lambda x: preprocess(x, tokenizer, format_hotpot_qa),
        batched=False,
    ).filter(lambda example: len(example) > 0)

    print(f"HotpotQA: {len(hotpot_train_tokenized)} train, {len(hotpot_val_tokenized)} val (after filtering)")

    # Task A: MATH
    raw_math = load_dataset(MATH_DATASET_NAME)
    total_math_samples_needed = N_TRAIN_EXAMPLES + N_VAL_EXAMPLES
    math_subset = raw_math["train"].shuffle(seed=42).select(range(total_math_samples_needed))
    val_size_fraction = N_VAL_EXAMPLES / total_math_samples_needed
    math_splits = math_subset.train_test_split(test_size=val_size_fraction, seed=42)
    math_train = math_splits["train"]
    math_val = math_splits["test"]

    print(f"Tokenizing and filtering MATH...")
    math_train_tokenized = math_train.filter(
        lambda x: filter_by_length(x, tokenizer, format_math),
        batched=False,
    ).map(
        lambda x: preprocess(x, tokenizer, format_math),
        batched=False,
    ).filter(lambda example: len(example) > 0)

    math_val_tokenized = math_val.filter(
        lambda x: filter_by_length(x, tokenizer, format_math),
        batched=False,
    ).map(
        lambda x: preprocess(x, tokenizer, format_math),
        batched=False,
    ).filter(lambda example: len(example) > 0)

    print(f"MATH: {len(math_train_tokenized)} train, {len(math_val_tokenized)} val (after filtering)")

    # --- „ÄêÂ∑≤ÂèçËΩ¨„ÄëExperiment 2: Sequential Training (CF) [MATH -> HotpotQA] ---
    print(f"\n--- Starting Experiment 2: Sequential Training (CF) [MATH -> HotpotQA] ---")

    # --- Phase 1: Train on MATH (or load from checkpoint) ---
    lora_config = get_lora_config()
    seq_model = get_peft_model(base_model, lora_config)
    seq_model.print_trainable_parameters()

    # seq_args_a = TrainingArguments(
    #     output_dir=os.path.join(RESULTS_DIR, "seq_training_A"),
    #     per_device_train_batch_size=PER_DEVICE_BS,
    #     gradient_accumulation_steps=GRAD_ACC_STEPS,
    #     num_train_epochs=TASK_A_EPOCHS,
    #     learning_rate=2e-4,
    #     # fp16=True, # <-- „ÄêBUG ‰øÆÂ§ç„ÄëÂà†Èô§Ê≠§Ë°å
    #     logging_steps=10,
    #     save_strategy="no",
    #     report_to="none",
    #     gradient_checkpointing=True,
    # )

    # seq_trainer_a = Trainer(
    #     model=seq_model,
    #     args=seq_args_a,
    #     train_dataset=math_train_tokenized, # <-- ËÆ≠ÁªÉ MATH
    #     eval_dataset=math_val_tokenized,
    #     data_collator=data_collator,
    # )

    # seq_trainer_a.train()

    # print(f"--- Phase 1 (MATH) training complete. Saving adapter to {TASK_A_ADAPTER_PATH} ---")
    # seq_model.save_pretrained(TASK_A_ADAPTER_PATH)
    # print("Adapter saved.")

    # del seq_trainer_a
    # torch.cuda.empty_cache()

    #  # --- Evaluate the "Task A Expert" model (whether trained or loaded) ---
    # print("\n--- Evaluating Model after Phase 1 (Task A Expert) ---")
    # eval_args = TrainingArguments(
    #     output_dir=os.path.join(RESULTS_DIR, "eval_temp"),
    #     per_device_eval_batch_size=PER_DEVICE_BS,
    #     # fp16=True, # <-- „ÄêBUG ‰øÆÂ§ç„ÄëÂà†Èô§Ê≠§Ë°å
    #     report_to="none",
    #     gradient_checkpointing=True,
    # )


    # eval_trainer = Trainer(
    #     model=seq_model,
    #     args=eval_args,
    #     data_collator=data_collator,
    # )

    # eval_hotpot_phase1 = eval_trainer.evaluate(eval_dataset=hotpot_val_tokenized)
    # print(f"  > Task B Expert - HotpotQA Val Loss: {eval_hotpot_phase1['eval_loss']:.4f}")
    # eval_math_phase1 = eval_trainer.evaluate(eval_dataset=math_val_tokenized)
    # print(f"  > Task A Expert - MATH Val Loss: {eval_math_phase1['eval_loss']:.4f}")
    # del eval_trainer, eval_args
    # torch.cuda.empty_cache()
    eval_hotpot_phase1 = {'eval_loss': []}
    eval_math_phase1 = {'eval_loss': []}
    import pdb
    pdb.set_trace()
    # # --- Phase 2: Train on HotpotQA (Forgetting MATH happens here) ---
    print(f"\n  --- Phase 2: Training on Task B (HotpotQA) ---")
    history = {"steps": [], "hotpot_loss": [], "math_loss": []}
    # Custom Trainer to log forgetting
    class ForgettingTrackerCallback(TrainerCallback):
        def __init__(self, hotpot_val, math_val, history_log, start_metrics):
            super().__init__()
            self.hotpot_eval_dataset = hotpot_val
            self.math_eval_dataset = math_val
            self.history = history_log
            self.trainer = None
            # --- „Äê‰øÆÂ§ç„Äë---
            # Ê∑ªÂä†‰∏Ä‰∏™ "ÈîÅ" Êù•Èò≤Ê≠¢Êó†ÈôêÈÄíÂΩí
            self.is_evaluating = False
            # ----------------
            # ËÆ∞ÂΩïÂàùÂßãÁä∂ÊÄÅ (Step 0)
            self.history["steps"].append(0)
            self.history["hotpot_loss"].append(start_metrics['hotpot_loss'])
            self.history["math_loss"].append(start_metrics['math_loss'])
            print("Initializing ForgettingTrackerCallback with starting metrics.")
        def set_trainer(self, trainer):
            """Âú® Trainer ‰æãÂåñÂêé, Ê≥®ÂÖ•ÂØπÂÆÉÁöÑÂºïÁî®„ÄÇ"""
            self.trainer = trainer
            print("Trainer reference set in callback.")

        def on_evaluate(self, args, state, control, **kwargs):
            import pdb
            pdb.set_trace()
            metrics = kwargs.get("metrics", None)
            if metrics is None:
                return control  # shouldn't happen normally

            step = state.global_step
            hotpot_loss = metrics["eval_hotpot_loss"]
            math_loss = metrics["eval_math_loss"]

            print(f"\nüî• Eval @ step {step}")
            print(f"  Hotpot Loss = {hotpot_loss:.4f}")
            print(f"  Math Loss   = {math_loss:.4f}")

            self.history["steps"].append(step)
            self.history["hotpot_loss"].append(hotpot_loss)
            self.history["math_loss"].append(math_loss)

            return control


    seq_args_b = TrainingArguments(
        output_dir=os.path.join(RESULTS_DIR, "seq_training_B"),
        per_device_train_batch_size=PER_DEVICE_BS,
        gradient_accumulation_steps=GRAD_ACC_STEPS,
        num_train_epochs=TASK_B_EPOCHS,
        learning_rate=7e-5,
        # logging_strategy="steps",
        # logging_steps=10,
        eval_strategy="steps",
        eval_steps=10,
        save_strategy="no",
        report_to=[],         # <-- ‰øùÊåÅËøô‰∏™ËÆæÁΩÆ
        # disable_tqdm=True,  # <-- ‰øùÊåÅËøô‰∏™ËÆæÁΩÆ
        gradient_checkpointing=True,
    )
    seq_model.enable_input_require_grads()
    # „Äê‰øÆÂ§ç 2„Äë: ÂÆû‰æãÂåñ *Êñ∞* ÁöÑ Callback
    forgetting_callback = ForgettingTrackerCallback(
        hotpot_val=hotpot_val_tokenized,
        math_val=math_val_tokenized,
        history_log=history,
        start_metrics={
            'hotpot_loss': eval_hotpot_phase1['eval_loss'],
            'math_loss': eval_math_phase1['eval_loss'],
        }
    )
    # „Äê‰øÆÂ§ç 3„Äë: ÂÆû‰æãÂåñ‰∏Ä‰∏™ *Ê†áÂáÜ* Trainer, Âπ∂‰º†ÂÖ•ÂõûË∞É
    seq_trainer_b = SmoothedLossTrainer(
        model=seq_model,
        args=seq_args_b,
        train_dataset=hotpot_train_tokenized,
        eval_dataset={              # Multi-dataset evaluation
            "hotpot": hotpot_val_tokenized,
            "math": math_val_tokenized,
        },
        data_collator=data_collator,
        callbacks=[forgetting_callback],  # <-- Âú®ËøôÈáå‰º†ÂÖ•ÂõûË∞É
        alpha=alpha
    )
    # „Äê‰øÆÂ§ç 4„Äë: Â∞Ü trainer ÂÆû‰æãÈìæÊé•ÂõûÂõûË∞É
    # (ÂõûË∞ÉÈúÄË¶ÅËøô‰∏™ÂºïÁî®Êù•Ë∞ÉÁî® self.trainer.evaluate())
    #tracker_callback.set_trainer(seq_trainer_b)

    import pdb
    pdb.set_trace()
    seq_trainer_b.train()

    # --- 5. Plot Results ---
    print("\n--- Saving History Data and Generating Plot ---")

    # --- ‰øùÂ≠ò history data Âà∞ JSON ---
    history_filename = os.path.join(RESULTS_DIR, "forgetting_history_MATH_to_HotpotQA_fp32.json")
    try:
        with open(history_filename, 'w') as f:
            json.dump(history, f, indent=4)
        print(f"History data saved to {history_filename}")
    except Exception as e:
        print(f"Error saving history to JSON: {e}")
    # --- [END] ---

    plt.figure(figsize=(12, 6))
    plt.plot(history["steps"], history["hotpot_loss"], 'o-', label="Task B (HotpotQA) Loss", color="blue")
    plt.plot(history["steps"], history["math_loss"], 'o-', label="Task A (MATH) Loss", color="red")

    plt.title(f"Catastrophic Forgetting: MATH -> HotpotQA (Model: {MODEL_NAME} FP32 LoRA)")
    plt.xlabel(f"Training Steps on Task B (HotpotQA) (Total Epochs: {TASK_B_EPOCHS})")
    plt.ylabel("Validation Loss")
    plt.legend()
    plt.grid(True)
    plt.yscale('log')
    plt.tight_layout()

    plot_filename = os.path.join(RESULTS_DIR, "sequential_forgetting_curve_MATH_to_HotpotQA_fp32.png")
    plt.savefig(plot_filename)
    print(f"Plot saved to {plot_filename}")

    try:
        from google.colab import files
        plt.show()
    except ImportError:
        print("Not in Colab, plot saved to file.")

if __name__ == "__main__":
    if not torch.cuda.is_available():
        print("ERROR: This experiment requires a GPU. Check Colab runtime type.")
    else:
        print(f"INFO: Running on GPU. VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
        if torch.cuda.get_device_properties(0).total_memory / 1e9 < 11:
            print("WARNING: VRAM is less than 11GB. You may hit OOM errors. Try lowering MAX_SEQ_LENGTH.")
    for alpha in alphas:
        main(alpha)