# UCB_CS182_project

In BP2 section, we conduct a two-phase fine-tuning experiment to examine how mixing a small proportion of Task 2 data (HotpotQA) into the earlier Task 1 training phase, and symmetrically mixing the same proportion of Task 1 data (Math) into the later Task 2 phase, affects the threshold of catastrophic forgetting. The overall fine-tuning order is fixed (Task 1 â†’ Task 2), but Phase 1 minibatches consist primarily of Math samples with a small ratio *r* of HotpotQA mixed in (we test *r* = 10%, 20%, 50%), while Phase 2 inverts this ratio so that HotpotQA dominates and Math forms the minority. Training in both phases uses identical model initialization, sampling procedures, and optimization hyperparameters,the training data in two phases are 4000 samples in total; Our experimental findings indicate that incorporating mixed data substantially mitigates catastrophic forgetting. In the first experiment, we set the mix ratio r=0.1, which means the training data in the first phase are 3600 Maths and 400 HotpotQA, and 3600 HotpotQA and 400 Maths in the second phase. The result after training is good, the validation loss of Maths in the second phase increase slowly. It substantially mitigates catastrophic forgetting compared to the baseline. Then we set the mix ratio to be 0.2 and 0.5. After training, we find that the higher we set the mix ratio, the less the validation loss of Maths is. However, the loss just reduce a little compared to the first experiment. We also draw a figure with the data derived from output. Through the figure we can see that the degree of mitigation increases as the proportion of mixed data becomes larger. Increasing the mixing ratio further continues to provide additional mitigation, but the improvements become marginal rather than substantial. In contrast, the transition from no mixing to any level of data mixing yields a pronounced reduction in catastrophic forgetting, highlighting that even a small amount of cross-task data substantially stabilizes performance.