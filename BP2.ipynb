{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da7dfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Untitled0.ipynb\n",
    "\n",
    "Automatically generated by Colab.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1J0nRtRnRbNiPlDiVxOQcGqhBl0xtBTUX\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    # BitsAndBytesConfig, # <-- has been removed\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers.trainer_callback import TrainerCallback\n",
    "# 【fixed】load PeftModel\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "HOTPOT_DATASET_NAME = \"hotpot_qa\"\n",
    "HOTPOT_DATASET_CONFIG = \"distractor\"\n",
    "MATH_DATASET_NAME = \"qwedsacf/competition_math\"\n",
    "RESULTS_DIR = \"./drive/MyDrive/\"\n",
    "\n",
    "\n",
    "# 【checkpoint  1 - control group】\n",
    "JOINT_ADAPTER_PATH = os.path.join(RESULTS_DIR, \"joint_adapter_llama_fp32\")\n",
    "\n",
    "# 【checkpoint  2 - experiment group】\n",
    "TASK_A_ADAPTER_PATH = os.path.join(RESULTS_DIR, \"math_adapter_llama_fp32\")\n",
    "\n",
    "\n",
    "# --- VRAM-Saving Config ---\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "#   Reduced batch size to fit larger models into limited VRAM\n",
    "PER_DEVICE_BS = 64\n",
    "GRAD_ACC_STEPS = 1 \n",
    "\n",
    "# --- Experiment Config ---\n",
    "N_TRAIN_EXAMPLES = 4000\n",
    "N_VAL_EXAMPLES = 400\n",
    "JOINT_EPOCHS = 2\n",
    "TASK_A_EPOCHS = 2 # For Math\n",
    "TASK_B_EPOCHS = 2 # For HotpotQA\n",
    "MIX_RATIOS = [0.9,0.8, 0.5]  # youcan change this to other ratios like 0.7, 0.3 to do more experiments\n",
    "\n",
    "# --- 2. Utility Functions (Data Formatting - Llama Chat Style) ---\n",
    "def format_hotpot_qa(example):\n",
    "    \"\"\"Formats HotpotQA data into a Llama-chat-style prompt.\"\"\"\n",
    "    context = \" \".join([\"\".join(s) for s in example[\"context\"][\"sentences\"]])\n",
    "    question = example[\"question\"]\n",
    "    answer = example[\"answer\"]\n",
    "\n",
    "    text = (\n",
    "        f\"<s>[INST] You are a helpful assistant. Use the following context to \"\n",
    "        f\"answer the question. Context: {context}\\n\\nQuestion: {question} [/INST] \"\n",
    "        f\"Answer: {answer}</s>\"\n",
    "    )\n",
    "    return text\n",
    "\n",
    "def format_math(example):\n",
    "    \"\"\"Formats MATH data into a Llama-chat-style prompt.\"\"\"\n",
    "    problem = example[\"problem\"]\n",
    "    solution = example[\"solution\"]\n",
    "\n",
    "    text = (\n",
    "        f\"<s>[INST] You are a math expert. Solve the following math problem. \"\n",
    "        f\"Show your work.\\nProblem: {problem} [/INST] \"\n",
    "        f\"Solution: {solution}</s>\"\n",
    "    )\n",
    "    return text\n",
    "\n",
    "def filter_by_length(example, tokenizer, formatter):\n",
    "    \"\"\"\n",
    "    only check the length. return True (keep) or False (discard).\n",
    "    \"\"\"\n",
    "    text = formatter(example)\n",
    "    tokenized = tokenizer(text, max_length=MAX_SEQ_LENGTH + 1, truncation=False, padding=False)\n",
    "    return len(tokenized['input_ids']) <= MAX_SEQ_LENGTH\n",
    "\n",
    "def preprocess(example, tokenizer, formatter):\n",
    "    \"\"\"\n",
    "    【fixed】\n",
    "    Format text, apply loss mask, and pad to max length.\n",
    "    \"\"\"\n",
    "    text = formatter(example)\n",
    "    tokenized = tokenizer(\n",
    "        text,\n",
    "        max_length=MAX_SEQ_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\", # fix ValueError\n",
    "    )\n",
    "    labels = tokenized[\"input_ids\"].copy()\n",
    "    inst_token_id = tokenizer.convert_tokens_to_ids(\"]\")\n",
    "\n",
    "    split_point = -1\n",
    "    for i in range(len(tokenized[\"input_ids\"]) - 1, -1, -1):\n",
    "        if tokenized[\"input_ids\"][i] == inst_token_id:\n",
    "            split_point = i + 1\n",
    "            break\n",
    "\n",
    "    if split_point == -1:\n",
    "        return {}\n",
    "\n",
    "    for i in range(split_point):\n",
    "        labels[i] = -100\n",
    "\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n",
    "\n",
    "# --- 3. Model Loading (【refactored】) ---\n",
    "\n",
    "def get_model_and_tokenizer_base():\n",
    "    \"\"\"\n",
    "    Only load the FP16 TinyLlama base model and tokenizer.\n",
    "    \"\"\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float16, # <-- use FP16 (save RAM), but training will be FP32\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    # Enable gradient checkpointing on the base model\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def get_lora_config():\n",
    "    \"\"\"\n",
    "    Only define the LoRA configuration.\n",
    "    \"\"\"\n",
    "    return LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"k_proj\",\n",
    "            \"v_proj\",\n",
    "            \"o_proj\",\n",
    "        ],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "def manual_evaluate(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Manually run evaluation on the given dataloader.\n",
    "    \"\"\"\n",
    "    model.eval()  # <--- Set to evaluation mode\n",
    "    total_loss = 0\n",
    "    total_steps = 0\n",
    "    with torch.no_grad(): # <--- Disable gradient calculation\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "            # Move batch to the device where the model is located\n",
    "            batch = {k: v.to(device) for k, v in batch.items() if k in [\"input_ids\", \"attention_mask\", \"labels\"]}\n",
    "\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_steps += 1\n",
    "\n",
    "    model.train() # <--- 【important】Set the model back to training mode\n",
    "    return total_loss / total_steps\n",
    "\n",
    "# --- 4. Main Experiment Logic ---\n",
    "def main(ratio):\n",
    "    if not os.path.exists(RESULTS_DIR):\n",
    "        os.makedirs(RESULTS_DIR)\n",
    "\n",
    "    print(f\"--- Loading Base Model & Tokenizer ---\")\n",
    "    base_model, tokenizer = get_model_and_tokenizer_base()\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    # --- Load and Process Datasets ---\n",
    "    print(f\"\\n--- Loading and Preprocessing Datasets (This may take a while) ---\")\n",
    "\n",
    "    # Task B: HotpotQA\n",
    "    raw_hotpot = load_dataset(HOTPOT_DATASET_NAME, HOTPOT_DATASET_CONFIG)\n",
    "    hotpot_train = raw_hotpot[\"train\"].shuffle(seed=42).select(range(N_TRAIN_EXAMPLES))\n",
    "    hotpot_val = raw_hotpot[\"validation\"].shuffle(seed=42).select(range(N_VAL_EXAMPLES))\n",
    "\n",
    "    print(f\"Tokenizing and filtering HotpotQA...\")\n",
    "    hotpot_train_tokenized = hotpot_train.filter(\n",
    "        lambda x: filter_by_length(x, tokenizer, format_hotpot_qa),\n",
    "        batched=False,\n",
    "    ).map(\n",
    "        lambda x: preprocess(x, tokenizer, format_hotpot_qa),\n",
    "        batched=False,\n",
    "    ).filter(lambda example: len(example) > 0)\n",
    "\n",
    "    hotpot_val_tokenized = hotpot_val.filter(\n",
    "        lambda x: filter_by_length(x, tokenizer, format_hotpot_qa),\n",
    "        batched=False,\n",
    "    ).map(\n",
    "        lambda x: preprocess(x, tokenizer, format_hotpot_qa),\n",
    "        batched=False,\n",
    "    ).filter(lambda example: len(example) > 0)\n",
    "\n",
    "    print(f\"HotpotQA: {len(hotpot_train_tokenized)} train, {len(hotpot_val_tokenized)} val (after filtering)\")\n",
    "\n",
    "    # Task A: MATH\n",
    "    raw_math = load_dataset(MATH_DATASET_NAME)\n",
    "    total_math_samples_needed = N_TRAIN_EXAMPLES + N_VAL_EXAMPLES\n",
    "    math_subset = raw_math[\"train\"].shuffle(seed=42).select(range(total_math_samples_needed))\n",
    "    val_size_fraction = N_VAL_EXAMPLES / total_math_samples_needed\n",
    "    math_splits = math_subset.train_test_split(test_size=val_size_fraction, seed=42)\n",
    "    math_train = math_splits[\"train\"]\n",
    "    math_val = math_splits[\"test\"]\n",
    "\n",
    "    print(f\"Tokenizing and filtering MATH...\")\n",
    "    math_train_tokenized = math_train.filter(\n",
    "        lambda x: filter_by_length(x, tokenizer, format_math),\n",
    "        batched=False,\n",
    "    ).map(\n",
    "        lambda x: preprocess(x, tokenizer, format_math),\n",
    "        batched=False,\n",
    "    ).filter(lambda example: len(example) > 0)\n",
    "\n",
    "    math_val_tokenized = math_val.filter(\n",
    "        lambda x: filter_by_length(x, tokenizer, format_math),\n",
    "        batched=False,\n",
    "    ).map(\n",
    "        lambda x: preprocess(x, tokenizer, format_math),\n",
    "        batched=False,\n",
    "    ).filter(lambda example: len(example) > 0)\n",
    "\n",
    "    print(f\"MATH: {len(math_train_tokenized)} train, {len(math_val_tokenized)} val (after filtering)\")\n",
    "\n",
    "     # Build mixed train dataset: keep total training size = N_TRAIN_EXAMPLES\n",
    "    math_count = int(round(N_TRAIN_EXAMPLES * ratio))\n",
    "    hotpot_count = max(1, N_TRAIN_EXAMPLES - math_count)  # ensure at least 1 from hotpot\n",
    "\n",
    "    # sample from tokenized datasets (they are datasets objects)\n",
    "    sampled_hotpot1 = hotpot_train_tokenized.shuffle(seed=42).select(range(min(len(hotpot_train_tokenized), hotpot_count)))\n",
    "    sampled_math1 = math_train_tokenized.shuffle(seed=42).select(range(min(len(math_train_tokenized), math_count)))\n",
    "    sampled_hotpot2 = hotpot_train_tokenized.shuffle(seed=42).select(range(min(len(hotpot_train_tokenized), math_count)))\n",
    "    sampled_math2 = math_train_tokenized.shuffle(seed=42).select(range(min(len(math_train_tokenized), hotpot_count)))\n",
    "    # concatenate to one training set and shuffle\n",
    "    math_train_tokenized = concatenate_datasets([sampled_hotpot1, sampled_math1]).shuffle(seed=42)\n",
    "\n",
    "    # concatenate to one training set and shuffle\n",
    "    hotpot_train_tokenized = concatenate_datasets([sampled_hotpot2, sampled_math2]).shuffle(seed=42)\n",
    "\n",
    "    print(f\"  Phase1 training: {len(math_train_tokenized)} samples (Hotpot: {len(sampled_hotpot1)}, MATH: {len(sampled_math1)})\")\n",
    "    print(f\"  Phase2 training: {len(hotpot_train_tokenized)} samples (Hotpot: {len(sampled_hotpot2)}, MATH: {len(sampled_math2)})\")\n",
    "\n",
    "\n",
    "\n",
    "    # --- 【reversed】Experiment 2: Sequential Training (CF) [MATH -> HotpotQA] ---\n",
    "    print(f\"\\n--- Starting Experiment 2: Sequential Training (CF) [MATH -> HotpotQA] ---\")\n",
    "\n",
    "    # --- Phase 1: Train on MATH (or load from checkpoint) ---\n",
    "    '''if os.path.exists(os.path.join(TASK_A_ADAPTER_PATH, \"adapter_model.safetensors\")):\n",
    "        print(f\"--- Found existing Task A (MATH) adapter. Loading from {TASK_A_ADAPTER_PATH} ---\")\n",
    "        seq_model = PeftModel.from_pretrained(base_model, TASK_A_ADAPTER_PATH)\n",
    "        print(\"Adapter loaded successfully.\")\n",
    "\n",
    "    else:'''\n",
    "    print(f\"--- No adapter found. Starting Phase 1: Training on Task A (MATH) ---\")\n",
    "    lora_config = get_lora_config()\n",
    "    seq_model = get_peft_model(base_model, lora_config)\n",
    "    seq_model.print_trainable_parameters()\n",
    "\n",
    "    seq_args_a = TrainingArguments(\n",
    "            output_dir=os.path.join(RESULTS_DIR, \"seq_training_A\"),\n",
    "            per_device_train_batch_size=PER_DEVICE_BS,\n",
    "            gradient_accumulation_steps=GRAD_ACC_STEPS,\n",
    "            num_train_epochs=TASK_A_EPOCHS,\n",
    "            learning_rate=2e-4,\n",
    "            # fp16=True,    # <-- 【BUG fix】delete this code\n",
    "            logging_steps=10,\n",
    "            save_strategy=\"no\",\n",
    "            report_to=\"none\",\n",
    "            gradient_checkpointing=True,\n",
    "    )\n",
    "\n",
    "    seq_trainer_a = Trainer(\n",
    "            model=seq_model,\n",
    "            args=seq_args_a,\n",
    "            train_dataset=math_train_tokenized, # <-- Train on MATH\n",
    "            eval_dataset=math_val_tokenized,\n",
    "            data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    seq_trainer_a.train()\n",
    "\n",
    "    print(f\"--- Phase 1 (MATH) training complete. Saving adapter to {TASK_A_ADAPTER_PATH} ---\")\n",
    "    seq_model.save_pretrained(TASK_A_ADAPTER_PATH)\n",
    "    print(\"Adapter saved.\")\n",
    "\n",
    "    del seq_trainer_a\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "     # --- Evaluate the \"Task A Expert\" model (whether trained or loaded) ---\n",
    "    print(\"\\n--- Evaluating Model after Phase 1 (Task A Expert) ---\")\n",
    "    eval_args = TrainingArguments(\n",
    "        output_dir=os.path.join(RESULTS_DIR, \"eval_temp\"),\n",
    "        per_device_eval_batch_size=PER_DEVICE_BS,\n",
    "        # fp16=True, # <-- 【BUG fix】delete this code\n",
    "        report_to=\"none\",\n",
    "        gradient_checkpointing=True,\n",
    "    )\n",
    "\n",
    "\n",
    "    eval_trainer = Trainer(\n",
    "        model=seq_model,\n",
    "        args=eval_args,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    eval_hotpot_phase1 = eval_trainer.evaluate(eval_dataset=hotpot_val_tokenized)\n",
    "    print(f\"  > Task B Expert - HotpotQA Val Loss: {eval_hotpot_phase1['eval_loss']:.4f}\")\n",
    "    eval_math_phase1 = eval_trainer.evaluate(eval_dataset=math_val_tokenized)\n",
    "    print(f\"  > Task A Expert - MATH Val Loss: {eval_math_phase1['eval_loss']:.4f}\")\n",
    "    del eval_trainer, eval_args\n",
    "    torch.cuda.empty_cache()\n",
    "    # --- Phase 2: Train on HotpotQA (Forgetting MATH happens here) ---\n",
    "    print(f\"\\n  --- Phase 2: Training on Task B (HotpotQA) ---\")\n",
    "    history = {\"steps\": [], \"hotpot_loss\": [], \"math_loss\": []}\n",
    "    # Custom Trainer to log forgetting\n",
    "    class ForgettingTrackerCallback(TrainerCallback):\n",
    "      def __init__(self, hotpot_val, math_val, history_log, start_metrics):\n",
    "          super().__init__()\n",
    "          self.hotpot_eval_dataset = hotpot_val\n",
    "          self.math_eval_dataset = math_val\n",
    "          self.history = history_log\n",
    "          self.trainer = None\n",
    "          # --- 【fix】---\n",
    "          # Add a \"lock\" to prevent infinite recursion\n",
    "          self.is_evaluating = False\n",
    "          # ----------------\n",
    "          # Record initial state (Step 0)\n",
    "          self.history[\"steps\"].append(0)\n",
    "          self.history[\"hotpot_loss\"].append(start_metrics['hotpot_loss'])\n",
    "          self.history[\"math_loss\"].append(start_metrics['math_loss'])\n",
    "          print(\"Initializing ForgettingTrackerCallback with starting metrics.\")\n",
    "      def set_trainer(self, trainer):\n",
    "          \"\"\"在 Trainer 例化后, 注入对它的引用。\"\"\"\n",
    "          self.trainer = trainer\n",
    "          print(\"Trainer reference set in callback.\")\n",
    "\n",
    "      def on_log(self, args, state, control, **kwargs):\n",
    "          \"\"\"Called when 'logging_steps' is triggered.\"\"\"\n",
    "          # --- 【fix 1】---\n",
    "          # If we are already in this function (due to recursive calls), exit immediately.\n",
    "          if self.is_evaluating:\n",
    "              return\n",
    "          # --- 【fix 2】---\n",
    "          # \"Acquire\" the lock\n",
    "          self.is_evaluating = True\n",
    "          # Ensure the trainer reference has been set\n",
    "          if not self.trainer:\n",
    "              print(\"WARNING: Trainer reference not set in callback, skipping eval.\")\n",
    "              self.is_evaluating = False # <-- Don't forget to release the lock here\n",
    "              return\n",
    "          print(f\"\\n--- Custom Eval at Step {state.global_step} ---\")\n",
    "          print(\"Evaluating on Task B (HotpotQA)...\")\n",
    "          # Use the trainer's evaluate method\n",
    "          hotpot_metrics = self.trainer.evaluate(eval_dataset=self.hotpot_eval_dataset)\n",
    "          hotpot_loss = hotpot_metrics['eval_loss']\n",
    "          print(f\"  > Step {state.global_step} - HotpotQA Val Loss: {hotpot_loss:.4f} (LEARNING?)\")\n",
    "          print(\"Evaluating on Task A (MATH)...\")\n",
    "          math_metrics = self.trainer.evaluate(eval_dataset=self.math_eval_dataset)\n",
    "          math_loss = math_metrics['eval_loss']\n",
    "          print(f\"  > Step {state.global_step} - MATH Val Loss: {math_loss:.4f} (FORGETTING?)\")\n",
    "          self.history[\"steps\"].append(state.global_step)\n",
    "          self.history[\"hotpot_loss\"].append(hotpot_loss)\n",
    "          self.history[\"math_loss\"].append(math_loss)\n",
    "          # --- 【fix 3】---\n",
    "          # \"Release\" the lock, so that the next on_log can run\n",
    "          self.is_evaluating = False\n",
    "          self.trainer.model.train()\n",
    "\n",
    "\n",
    "    seq_args_b = TrainingArguments(\n",
    "        output_dir=os.path.join(RESULTS_DIR, \"seq_training_B\"),\n",
    "        per_device_train_batch_size=PER_DEVICE_BS,\n",
    "        gradient_accumulation_steps=GRAD_ACC_STEPS,\n",
    "        num_train_epochs=TASK_B_EPOCHS,\n",
    "        learning_rate=7e-5,\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"no\",\n",
    "        report_to=[],         # <-- Keep this setting\n",
    "        # disable_tqdm=True,  # <-- Keep this setting\n",
    "        gradient_checkpointing=True,\n",
    "    )\n",
    "    seq_model.enable_input_require_grads()\n",
    "    # 【fix 2】: Instantiate a *new* Callback\n",
    "    tracker_callback = ForgettingTrackerCallback(\n",
    "        hotpot_val=hotpot_val_tokenized,\n",
    "        math_val=math_val_tokenized,\n",
    "        history_log=history,\n",
    "        start_metrics={\n",
    "            'hotpot_loss': eval_hotpot_phase1['eval_loss'],\n",
    "            'math_loss': eval_math_phase1['eval_loss'],\n",
    "        }\n",
    "    )\n",
    "    # 【fix 3】: Instantiate a *standard* Trainer, and pass in the callback\n",
    "    seq_trainer_b = Trainer(\n",
    "        model=seq_model,\n",
    "        args=seq_args_b,\n",
    "        train_dataset=hotpot_train_tokenized,\n",
    "        eval_dataset=hotpot_val_tokenized,\n",
    "        data_collator=data_collator,\n",
    "        callbacks=[tracker_callback]  # <-- Pass the callback here\n",
    "    )\n",
    "    # 【fix 4】: Link the trainer instance back to the callback\n",
    "    # (The callback needs this reference to call self.trainer.evaluate())\n",
    "    tracker_callback.set_trainer(seq_trainer_b)\n",
    "    seq_trainer_b.train()\n",
    "\n",
    "    # --- 5. Plot Results ---\n",
    "    print(\"\\n--- Saving History Data and Generating Plot ---\")\n",
    "\n",
    "    # --- Save history data to JSON ---\n",
    "    history_filename = os.path.join(RESULTS_DIR, f\"forgetting_curve_MATH_mix_{int(ratio*100)}.json\")\n",
    "    try:\n",
    "        with open(history_filename, 'w') as f:\n",
    "            json.dump(history, f, indent=4)\n",
    "        print(f\"History data saved to {history_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving history to JSON: {e}\")\n",
    "    # --- [END] ---\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(history[\"steps\"], history[\"hotpot_loss\"], 'o-', label=\"Task B (HotpotQA) Loss\", color=\"blue\")\n",
    "    plt.plot(history[\"steps\"], history[\"math_loss\"], 'o-', label=\"Task A (MATH) Loss\", color=\"red\")\n",
    "\n",
    "    plt.title(f\"Forgetting Curve (mix {int(ratio*100)}% MATH in Phase2)\")\n",
    "    plt.xlabel(f\"Training Steps on Task B (HotpotQA) (Total Epochs: {TASK_B_EPOCHS})\")\n",
    "    plt.ylabel(\"Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.yscale('log')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plot_filename = os.path.join(RESULTS_DIR, f\"forgetting_curve_MATH_mix_{int(ratio*100)}.png\")\n",
    "    plt.savefig(plot_filename)\n",
    "    print(f\"Plot saved to {plot_filename}\")\n",
    "    \n",
    "    # cleanup GPU memory between runs\n",
    "    del seq_trainer_b\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        plt.show()\n",
    "    except ImportError:\n",
    "        print(\"Not in Colab, plot saved to file.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"ERROR: This experiment requires a GPU. Check Colab runtime type.\")\n",
    "    else:\n",
    "        print(f\"INFO: Running on GPU. VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "        if torch.cuda.get_device_properties(0).total_memory / 1e9 < 11:\n",
    "            print(\"WARNING: VRAM is less than 11GB. You may hit OOM errors. Try lowering MAX_SEQ_LENGTH.\")\n",
    "    for ratio in MIX_RATIOS:\n",
    "        main(ratio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
