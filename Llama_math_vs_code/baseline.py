# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J0nRtRnRbNiPlDiVxOQcGqhBl0xtBTUX
"""

import torch
import json
import os
import matplotlib.pyplot as plt
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
)
from datasets import load_dataset, concatenate_datasets
from transformers.trainer_callback import TrainerCallback
from peft import LoraConfig, get_peft_model, PeftModel
from tqdm import tqdm
import warnings
from torch.utils.data import DataLoader
from torch.optim import AdamW

# Suppress warnings
warnings.filterwarnings("ignore")

# --- 1. Configuration ---
MODEL_NAME = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

# Task B: CodeParrot
CODE_DATASET_NAME = "codeparrot/codeparrot-clean"
CODE_DATASET_CONFIG = None

# Task A: MATH
MATH_DATASET_NAME = "qwedsacf/competition_math"

RESULTS_DIR = "./drive/MyDrive/"

# 【Checkpoints】
# Task A Adapter (MATH)
TASK_A_ADAPTER_PATH = os.path.join(RESULTS_DIR, "math_adapter_llama_fp32")


# --- VRAM-Saving Config ---
MAX_SEQ_LENGTH = 2048
PER_DEVICE_BS = 64
GRAD_ACC_STEPS = 1 

# --- Experiment Config ---
N_TRAIN_EXAMPLES = 4000
N_VAL_EXAMPLES = 400
TASK_A_EPOCHS = 2 # For Math
TASK_B_EPOCHS = 2 # For CodeParrot

# --- 2. Utility Functions (Data Formatting) ---
def format_codeparrot(example):
    """Formats CodeParrot data into a Llama-chat-style prompt."""
    code = example["content"]
    # Truncate extremely long code files to avoid issues before tokenization if needed,
    # but tokenizer truncation handles most.
    text = (
        f"<s>[INST] You are an expert programmer. Write python code. [/INST] "
        f"{code}</s>"
    )
    return text

def format_math(example):
    """Formats MATH data into a Llama-chat-style prompt."""
    problem = example["problem"]
    solution = example["solution"]

    text = (
        f"<s>[INST] You are a math expert. Solve the following math problem. "
        f"Show your work.\nProblem: {problem} [/INST] "
        f"Solution: {solution}</s>"
    )
    return text

def filter_by_length(example, tokenizer, formatter):
    text = formatter(example)
    tokenized = tokenizer(text, max_length=MAX_SEQ_LENGTH + 1, truncation=False, padding=False)
    return len(tokenized['input_ids']) <= MAX_SEQ_LENGTH

def preprocess(example, tokenizer, formatter):
    text = formatter(example)
    tokenized = tokenizer(
        text,
        max_length=MAX_SEQ_LENGTH,
        truncation=True,
        padding="max_length",
    )
    labels = tokenized["input_ids"].copy()
    inst_token_id = tokenizer.convert_tokens_to_ids("]")

    split_point = -1
    for i in range(len(tokenized["input_ids"]) - 1, -1, -1):
        if tokenized["input_ids"][i] == inst_token_id:
            split_point = i + 1
            break

    if split_point == -1:
        return {}

    for i in range(split_point):
        labels[i] = -100

    tokenized["labels"] = labels
    return tokenized

# --- 3. Model Loading ---

def get_model_and_tokenizer_base():
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        torch_dtype=torch.float16, 
        device_map="auto",
        trust_remote_code=True,
    )
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"
    model.gradient_checkpointing_enable()
    return model, tokenizer

def get_lora_config():
    return LoraConfig(
        r=8,
        lora_alpha=16,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM",
    )

# --- 4. Main Experiment Logic ---
def main():
    if not os.path.exists(RESULTS_DIR):
        os.makedirs(RESULTS_DIR)

    print(f"--- Loading Base Model & Tokenizer ---")
    base_model, tokenizer = get_model_and_tokenizer_base()

    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

    # --- Load and Process Datasets ---
    print(f"\n--- Loading and Preprocessing Datasets (This may take a while) ---")

    # Task B: CodeParrot
    print(f"Loading CodeParrot (subset)...")
    # Load a subset directly to avoid downloading the whole TB dataset
    # REMOVED trust_remote_code=True
    raw_code_dataset = load_dataset(CODE_DATASET_NAME, CODE_DATASET_CONFIG, split="train[:20000]")
    
    # Split into Train/Val
    code_splits = raw_code_dataset.train_test_split(test_size=N_VAL_EXAMPLES, train_size=N_TRAIN_EXAMPLES, seed=42)
    code_train = code_splits["train"]
    code_val = code_splits["test"]

    print(f"Tokenizing and filtering CodeParrot...")
    code_train_tokenized = code_train.filter(
        lambda x: filter_by_length(x, tokenizer, format_codeparrot),
        batched=False,
    ).map(
        lambda x: preprocess(x, tokenizer, format_codeparrot),
        batched=False,
    ).filter(lambda example: len(example) > 0)

    code_val_tokenized = code_val.filter(
        lambda x: filter_by_length(x, tokenizer, format_codeparrot),
        batched=False,
    ).map(
        lambda x: preprocess(x, tokenizer, format_codeparrot),
        batched=False,
    ).filter(lambda example: len(example) > 0)

    print(f"CodeParrot: {len(code_train_tokenized)} train, {len(code_val_tokenized)} val (after filtering)")

    # Task A: MATH
    raw_math = load_dataset(MATH_DATASET_NAME)
    total_math_samples_needed = N_TRAIN_EXAMPLES + N_VAL_EXAMPLES
    math_subset = raw_math["train"].shuffle(seed=42).select(range(total_math_samples_needed))
    val_size_fraction = N_VAL_EXAMPLES / total_math_samples_needed
    math_splits = math_subset.train_test_split(test_size=val_size_fraction, seed=42)
    math_train = math_splits["train"]
    math_val = math_splits["test"]

    print(f"Tokenizing and filtering MATH...")
    math_train_tokenized = math_train.filter(
        lambda x: filter_by_length(x, tokenizer, format_math),
        batched=False,
    ).map(
        lambda x: preprocess(x, tokenizer, format_math),
        batched=False,
    ).filter(lambda example: len(example) > 0)

    math_val_tokenized = math_val.filter(
        lambda x: filter_by_length(x, tokenizer, format_math),
        batched=False,
    ).map(
        lambda x: preprocess(x, tokenizer, format_math),
        batched=False,
    ).filter(lambda example: len(example) > 0)

    print(f"MATH: {len(math_train_tokenized)} train, {len(math_val_tokenized)} val (after filtering)")


    # --- Experiment 2: Sequential Training (CF) [MATH -> CodeParrot] ---
    print(f"\n--- Starting Experiment 2: Sequential Training (CF) [MATH -> CodeParrot] ---")

    # --- Phase 1: Train on MATH (or load from checkpoint) ---
    if os.path.exists(os.path.join(TASK_A_ADAPTER_PATH, "adapter_model.safetensors")):
        print(f"--- Found existing Task A (MATH) adapter. Loading from {TASK_A_ADAPTER_PATH} ---")
        seq_model = PeftModel.from_pretrained(base_model, TASK_A_ADAPTER_PATH)
        print("Adapter loaded successfully.")

    else:
        print(f"--- No adapter found. Starting Phase 1: Training on Task A (MATH) ---")
        lora_config = get_lora_config()
        seq_model = get_peft_model(base_model, lora_config)
        seq_model.print_trainable_parameters()

        seq_args_a = TrainingArguments(
            output_dir=os.path.join(RESULTS_DIR, "seq_training_A"),
            per_device_train_batch_size=PER_DEVICE_BS,
            gradient_accumulation_steps=GRAD_ACC_STEPS,
            num_train_epochs=TASK_A_EPOCHS,
            learning_rate=2e-4,
            logging_steps=10,
            save_strategy="no",
            report_to="none",
            gradient_checkpointing=True,
        )

        seq_trainer_a = Trainer(
            model=seq_model,
            args=seq_args_a,
            train_dataset=math_train_tokenized, 
            eval_dataset=math_val_tokenized,
            data_collator=data_collator,
        )

        seq_trainer_a.train()

        print(f"--- Phase 1 (MATH) training complete. Saving adapter to {TASK_A_ADAPTER_PATH} ---")
        seq_model.save_pretrained(TASK_A_ADAPTER_PATH)
        print("Adapter saved.")

        del seq_trainer_a
        torch.cuda.empty_cache()

     # --- Evaluate the "Task A Expert" model ---
    print("\n--- Evaluating Model after Phase 1 (Task A Expert) ---")
    eval_args = TrainingArguments(
        output_dir=os.path.join(RESULTS_DIR, "eval_temp"),
        per_device_eval_batch_size=PER_DEVICE_BS,
        report_to="none",
        gradient_checkpointing=True,
    )

    eval_trainer = Trainer(
        model=seq_model,
        args=eval_args,
        data_collator=data_collator,
    )

    eval_code_phase1 = eval_trainer.evaluate(eval_dataset=code_val_tokenized)
    print(f"  > Task B (Code) Val Loss: {eval_code_phase1['eval_loss']:.4f}")
    eval_math_phase1 = eval_trainer.evaluate(eval_dataset=math_val_tokenized)
    print(f"  > Task A (MATH) Val Loss: {eval_math_phase1['eval_loss']:.4f}")
    del eval_trainer, eval_args
    torch.cuda.empty_cache()
    
    # --- Phase 2: Train on CodeParrot (Forgetting MATH happens here) ---
    print(f"\n  --- Phase 2: Training on Task B (CodeParrot) ---")
    history = {"steps": [], "code_loss": [], "math_loss": []}
    
    # Custom Trainer to log forgetting
    class ForgettingTrackerCallback(TrainerCallback):
      def __init__(self, code_val, math_val, history_log, start_metrics):
          super().__init__()
          self.code_eval_dataset = code_val
          self.math_eval_dataset = math_val
          self.history = history_log
          self.trainer = None
          self.is_evaluating = False

          # Initial State (Step 0)
          self.history["steps"].append(0)
          self.history["code_loss"].append(start_metrics['code_loss'])
          self.history["math_loss"].append(start_metrics['math_loss'])
          print("Initializing ForgettingTrackerCallback with starting metrics.")
      
      def set_trainer(self, trainer):
          self.trainer = trainer

      def on_log(self, args, state, control, **kwargs):
          if self.is_evaluating:
              return
          self.is_evaluating = True
          
          if not self.trainer:
              self.is_evaluating = False
              return
              
          print(f"\n--- Custom Eval at Step {state.global_step} ---")
          print("Evaluating on Task B (CodeParrot)...")
          code_metrics = self.trainer.evaluate(eval_dataset=self.code_eval_dataset)
          code_loss = code_metrics['eval_loss']
          print(f"  > Step {state.global_step} - CodeParrot Val Loss: {code_loss:.4f} (LEARNING?)")
          
          print("Evaluating on Task A (MATH)...")
          math_metrics = self.trainer.evaluate(eval_dataset=self.math_eval_dataset)
          math_loss = math_metrics['eval_loss']
          print(f"  > Step {state.global_step} - MATH Val Loss: {math_loss:.4f} (FORGETTING?)")
          
          self.history["steps"].append(state.global_step)
          self.history["code_loss"].append(code_loss)
          self.history["math_loss"].append(math_loss)
          
          self.is_evaluating = False
          self.trainer.model.train()


    seq_args_b = TrainingArguments(
        output_dir=os.path.join(RESULTS_DIR, "seq_training_B"),
        per_device_train_batch_size=PER_DEVICE_BS,
        gradient_accumulation_steps=GRAD_ACC_STEPS,
        num_train_epochs=TASK_B_EPOCHS,
        learning_rate=7e-5,
        logging_steps=10,
        save_strategy="no",
        report_to=[],
        gradient_checkpointing=True,
    )
    seq_model.enable_input_require_grads()
    
    tracker_callback = ForgettingTrackerCallback(
        code_val=code_val_tokenized,
        math_val=math_val_tokenized,
        history_log=history,
        start_metrics={
            'code_loss': eval_code_phase1['eval_loss'],
            'math_loss': eval_math_phase1['eval_loss'],
        }
    )

    seq_trainer_b = Trainer(
        model=seq_model,
        args=seq_args_b,
        train_dataset=code_train_tokenized,
        eval_dataset=code_val_tokenized,
        data_collator=data_collator,
        callbacks=[tracker_callback]
    )

    tracker_callback.set_trainer(seq_trainer_b)
    seq_trainer_b.train()

    # --- 5. Plot Results ---
    print("\n--- Saving History Data and Generating Plot ---")

    history_filename = os.path.join(RESULTS_DIR, "forgetting_history_MATH_to_Code_fp32.json")
    try:
        with open(history_filename, 'w') as f:
            json.dump(history, f, indent=4)
        print(f"History data saved to {history_filename}")
    except Exception as e:
        print(f"Error saving history to JSON: {e}")

    plt.figure(figsize=(12, 6))
    plt.plot(history["steps"], history["code_loss"], 'o-', label="Task B (CodeParrot) Loss", color="blue")
    plt.plot(history["steps"], history["math_loss"], 'o-', label="Task A (MATH) Loss", color="red")

    plt.title(f"Catastrophic Forgetting: MATH -> CodeParrot (Model: {MODEL_NAME} FP32 LoRA)")
    plt.xlabel(f"Training Steps on Task B (CodeParrot) (Total Epochs: {TASK_B_EPOCHS})")
    plt.ylabel("Validation Loss")
    plt.legend()
    plt.grid(True)
    plt.yscale('log')
    plt.tight_layout()

    plot_filename = os.path.join(RESULTS_DIR, "sequential_forgetting_curve_MATH_to_Code_fp32.png")
    plt.savefig(plot_filename)
    print(f"Plot saved to {plot_filename}")

    try:
        from google.colab import files
        plt.show()
    except ImportError:
        print("Not in Colab, plot saved to file.")

if __name__ == "__main__":
    if not torch.cuda.is_available():
        print("ERROR: This experiment requires a GPU. Check Colab runtime type.")
    else:
        print(f"INFO: Running on GPU. VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
        if torch.cuda.get_device_properties(0).total_memory / 1e9 < 11:
            print("WARNING: VRAM is less than 11GB. You may hit OOM errors. Try lowering MAX_SEQ_LENGTH.")
    main()